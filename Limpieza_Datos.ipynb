{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835fe7ac-9489-4987-a172-cbc4e2f8a24f",
   "metadata": {},
   "source": [
    "![Banner_Visualización&Storytelling.png](Banner_Visualización&Storytelling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee25b6-ffe2-45c6-b3a2-604e7a0e31d4",
   "metadata": {},
   "source": [
    "## Dashboard 2: visualización y documentación para narrar a un equipo de trabajo\n",
    "### Modelar los datos \n",
    "\n",
    "En el notebook, se realizarán los siguientes pasos de modelado: \n",
    "\n",
    "1. Extraer la base de datos “Ask A Manager Salary Survey 2021” de AskAManager.org (https://docs.google.com/spreadsheets/d/1IPS5dBSGtwYVbjsfbaMCYIWnOuRmJcbequohNxCyGVw/edit?resourcekey#gid=1625408792)  \n",
    "\n",
    "2. Limpiar los campos de “Country” y “City” para homogenizar los nombres de los lugares. \n",
    "\n",
    "3. Crear 2 campos nuevos: “salario_anual” y “compensaciones” convirtiendo sueldos y compensaciones a Pesos Colombianos basados en la tasa de cambio del día que hacen el ejercicio. \n",
    "\n",
    "4. Crear un campo adicional sumando salario anual y compensaciones en pesos colombianos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7928bc1-cd74-4419-8db6-c18e42850681",
   "metadata": {},
   "source": [
    "### Librerías a importar\n",
    "\n",
    "A continuación, se muestran las librerias a importar para poder implementar los procedimientos de este notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea6cce3-b43b-4d7b-bb48-42ce682d884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataprep.clean import clean_country\n",
    "import warnings\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6189b7-6b54-484c-9a68-2cea5cfa0e43",
   "metadata": {},
   "source": [
    "### 1. Importar la base de datos\n",
    "\n",
    "Se emplea la libreria de pandas para importar el archivo de datos a la sesión de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e8c2b1-c654-409e-a883-56bf065ffe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Ask A Manager Salary Survey 2021 (Responses) - Form Responses 1.csv\", thousands=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b9cc4b-5577-4d5c-a530-0270806fbd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de observaciones en la base de datos: 27936\n",
      "Número de columnas en la base de datos: 18\n"
     ]
    }
   ],
   "source": [
    "#Tamaño del dataset\n",
    "print(f\"Número de observaciones en la base de datos: {df.shape[0]}\")\n",
    "print(f\"Número de columnas en la base de datos: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe3868-4afb-4b90-b04a-12221f0628e3",
   "metadata": {},
   "source": [
    "#### Renombrar las columnas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5035c312-ade9-47a0-ab65-e6df3a3003e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'age_range', 'industry', 'job_title',\n",
      "       'additional_context_job', 'annual_salary',\n",
      "       'additional_monetary_compensation', 'currency', 'other_currency',\n",
      "       'additional_context_income', 'country', 'state_US', 'city',\n",
      "       'years_work_experience', 'years_work_experience_field',\n",
      "       'level_education', 'gender', 'race'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Old column names\n",
    "old_col_names = df.columns.values\n",
    "\n",
    "# New column names\n",
    "new_col_names = ['timestamp', 'age_range','industry','job_title','additional_context_job','annual_salary','additional_monetary_compensation','currency','other_currency','additional_context_income','country','state_US','city','years_work_experience','years_work_experience_field','level_education','gender','race']\n",
    "\n",
    "# Create a dictionary mapping old column names to new column names\n",
    "col_name_map = {old: new for old, new in zip(old_col_names, new_col_names)}\n",
    "\n",
    "# Use the rename method to update the column names\n",
    "df.rename(columns=col_name_map, inplace=True)\n",
    "\n",
    "# Verify that the columns have been renamed\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c6797-2959-42a9-a4b7-6b46b5c326fa",
   "metadata": {},
   "source": [
    "### 2. Limpieza de los campos geográficos \n",
    "\n",
    "A continuación, se realiza la limpieza de los campos de “Country” y “City” para homogenizar los nombres de los lugares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba2951-609b-4a2d-937c-da386d6f5677",
   "metadata": {},
   "source": [
    "Primero, se realiza la limpieza del campo **country**. Se tiene en cuenta la función *clean_country* del paquete *dataprep* para la estandarización del campo mencionado pues la función limpia una columna que contiene nombres de países y/o códigos de país ISO 3166, y los normaliza en el formato deseado. Sin embargo, hay nombres de paises que no son resultos por la función pues tienen errores de escritura. De esta manera, se realiza una limpieza manual usando un diccionario con solo los textos faltantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec054889-13c7-437d-9a70-aee3b5ca6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dict = {'UK': 'U.K.', 'Scotland ': 'U.K.', 'England': 'U.K.','ENGLAND': 'U.K.', 'England ': 'U.K.', 'Scotland': 'U.K.', 'Uk': 'U.K.', 'England/UK': 'U.K.',\n",
    "                'U.S>': 'USA', 'ISA': 'USA', 'United State': 'USA', 'America': 'USA', 'United State of America': 'USA', 'United Statws': 'USA', 'U.S': 'USA',\n",
    "                'Unites States': 'USA', 'U. S. ': 'USA', 'United Sates': 'USA', 'Uniited States': 'USA', 'United Sates of America': 'USA',  'Unted States': 'USA',\n",
    "                'United Stattes': 'USA', 'United Statea': 'USA', 'United Statea': 'USA', 'Unites States': 'USA', 'United Statees': 'USA', 'Uniyed states': 'USA', \n",
    "                'Uniyes States': 'USA', 'U.A.': 'USA', 'U. S.': 'USA', ' US of A': 'USA', 'United Kindom': 'U.K.', 'United Status': 'USA', 'Uniteed States': 'USA',\n",
    "                'United Stares': 'USA', 'United Stares': 'USA', 'Unites states ': 'USA', 'The US': 'USA', 'UnitedStates': 'USA', 'United statew': 'USA',\n",
    "                'United Statues': 'USA', 'United Statues': 'USA', 'Untied States': 'USA',  'Unitied States': 'USA', ' United Sttes': 'USA',\n",
    "                'united stated': 'USA', ' Uniter Statez': 'USA', 'U. S ': 'USA', 'United Stateds': 'USA', 'Unitef Stated': 'USA',\n",
    "                'United Stares ': 'USA', 'USaa': 'USA', 'america': 'USA', 'United Statss': 'USA', 'United  States': 'USA','United Stated\t': 'USA','Northern Ireland\t': \"Ireland\"}\n",
    "\n",
    "df['country'] = df['country'].map(country_dict).fillna(df['country'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89969f35-7b16-4b6f-9332-4953338bf8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                            | 0/8 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Cleaning Report:\n",
      "\t15489 values cleaned (55.44%)\n",
      "\t133 values unable to be parsed (0.48%), set to NaN\n",
      "Result contains 27803 (99.52%) values in the correct format and 133 null values (0.48%)\n"
     ]
    }
   ],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = clean_country(df, 'country', output_format= \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e3062-a181-4cc7-bb66-e7d11d69c204",
   "metadata": {},
   "source": [
    "Cabe mencionar que el diccionario solo cuenta con los nombres de los paises que no son posibles de traducir por la función. Lo anterior, nos apoya en el proceso de limpieza pues no es necesario realizar una limpieza manual a muchos registros sino solo aquellos que la función no leyo. Hay de los registros originales de la base de datos solo un 0.48% (133 registros) no fueron estandarizados.\n",
    "\n",
    "Finalemente, se crea una nueva variable **Country_clean** con los países estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f87c7a-a023-4770-a7bc-92e1f1c0fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cdf690-61c9-415b-bd00-8ff32690bab6",
   "metadata": {},
   "source": [
    "Ahora, se realiza la limpieza del campo **city**. Primero, se eliminar las filas que presenten ciudades en valores faltantes. Luego, se revisan y organizan las ciudades en una lista para aplicar la métrica de similitud de textos del paquete *fuzzywuzzy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50f45ce8-544b-4105-9d8c-81076cc4c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = df_clean[\"city\"].tolist()\n",
    "df_cities = [str(x) for x in df_cities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a9d2a2-c67e-4dfd-acad-501176d4daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(strings):\n",
    "    return [string.strip() for string in strings]\n",
    "city_no_spaces = remove_spaces(df_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cfb1748-2cf5-45d3-bde0-a361a103998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(lst):\n",
    "    return list(set(lst))\n",
    "unique_cities = get_unique_values(city_no_spaces)\n",
    "sorted_unique_cities = sorted(unique_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67773f05-b1d5-486c-aaf2-c1a5df1b3940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4216"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_unique_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e8f0c7-2f8c-4183-8e03-5bbbb06487ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '-']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '--']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '---']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '-----']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '.']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '..']\n",
      "Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '/']\n"
     ]
    }
   ],
   "source": [
    "score_sort = [(x,) + i\n",
    "             for x in sorted_unique_cities\n",
    "             for i in process.extract(x, sorted_unique_cities, scorer=fuzz.token_set_ratio)]\n",
    "#Create a dataframe from the tuples\n",
    "similarity_sort = pd.DataFrame(score_sort, columns=['city_sort','match_sort','score_sort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec0d752-6e23-49c3-ab89-6e706c49b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_sort['sorted_city_sort'] = np.minimum(similarity_sort['city_sort'], similarity_sort['match_sort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b6d8f08-b1cf-4c5d-9c4d-bc550631ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_score_sort = similarity_sort[(similarity_sort['score_sort'] >= 98) &\n",
    "                (similarity_sort['city_sort'] !=  similarity_sort['match_sort']) &\n",
    "                (similarity_sort['sorted_city_sort'] != similarity_sort['match_sort'])]\n",
    "high_score_sort = high_score_sort.drop('sorted_city_sort',axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a90a93d4-5128-43de-b73f-5774ded8fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_clean.iterrows():\n",
    "  c = high_score_sort[high_score_sort['match_sort'] == row['city']]\n",
    "  if c.empty == False:\n",
    "    df_clean.loc[index, 'city_clean'] = c.iloc[0]['city_sort']\n",
    "  else:\n",
    "    df_clean.loc[index, 'city_clean'] = row['city']\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4ce570-4c90-4583-ac15-6cb684d0e630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3728"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se reduce la cantidad de valores unicos de la columna City\n",
    "len(df_clean['city_clean'].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b1807-3a97-4708-8a44-b7855ee71e37",
   "metadata": {},
   "source": [
    "Aplicando la métrica de similitud de textos, logramos disminuir la cantidad de países únicos en aproximadamente 500. Finalmente, se crea un nuevo campo llamado *city_clean* donde estan las ciudades estandarizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e742c22-4429-43d5-a28f-a9bb8b420c92",
   "metadata": {},
   "source": [
    "### 3. Crear campos nuevos: “salario_anual” y “compensaciones” \n",
    "\n",
    "Para crear los campos mencionados, se tuvo en cuenta la conversión de sueldos y compensaciones a Pesos Colombianos basados en la tasa de cambio del día 12/02/2023. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e4769e8-c8d4-4cfd-b2f3-d69d31169f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEK',\n",
       " 'CAD',\n",
       " 'JPY',\n",
       " 'Other',\n",
       " 'CHF',\n",
       " 'AUD/NZD',\n",
       " 'USD',\n",
       " 'HKD',\n",
       " 'ZAR',\n",
       " 'EUR',\n",
       " 'GBP']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se revisan las divisas que aparecen\n",
    "def get_unique_values_of_column(df, column_name):\n",
    "    column = df[column_name].tolist()\n",
    "    return list(set(column))\n",
    "unique_currency = get_unique_values_of_column(df_clean,\"currency\")\n",
    "unique_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39122ee3-da70-46c4-9355-e69faf32af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipo de cambio obtenido el 12/02/2023 por https://www.xe.com/currencyconverter/\n",
    "currency_dict = {'USD': 4803.71, 'AUD/NZD': 3020.89, 'GBP': 5782.32, 'CHF': 5187.04,'HKD': 611.47,'EUR': 5119.48,'SEK': 457.31, 'CAD': 3586.98, 'Other': 4803.71,  \n",
    "                 'ZAR': 267.51,  'JPY': 36.40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53875ccc-7fa2-4609-a559-5fccbc09fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir a pesos colombianos\n",
    "df_clean['salario_anual'] = df_clean.apply(lambda row: row['annual_salary']*currency_dict[row['currency']], axis=1)\n",
    "df_clean['compensaciones'] = df_clean.apply(lambda row: row['additional_monetary_compensation']*currency_dict[row['currency']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c178d-e77d-4157-929f-e20f6c36f502",
   "metadata": {},
   "source": [
    "### 4. Crear un campo adicional del salario total en pesos colombianos \n",
    "\n",
    "A continuación, se crea el campo adicional sumando salario anual y compensaciones en pesos colombianos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5067bda9-b5d5-4ad6-aba4-b345e60314f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['ingreso_total'] = df_clean.apply(lambda row:  row['salario_anual'] + row['compensaciones'] if ~np.isnan(row['compensaciones']) else row['salario_anual'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03db0a2-6102-495a-9e20-c081a4a47cfb",
   "metadata": {},
   "source": [
    "### Guardar archivo con el modelamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa05c63f-c1a0-4c33-b1fa-eea7212586de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_excel(\"V&S_Encuestas_clean.xlsx\", index=False, encoding=\"UTF-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
